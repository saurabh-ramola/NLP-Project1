{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Tokenising 100 Files in the Corpus sentence-wise first and then word-wise.<h2/>**\n",
    "<h3>All the tokenising into sentences and word-tokens is done in the below cell.Furthermore for spelling prediction I have made spelling n-grams as well in the below-cell.<br/>\n",
    "After that I have plotted the top-10 and bottom-10 occurring unigrams in a table.</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            [
             "the",
             435046
            ],
            [
             "and",
             242206
            ],
            [
             "of",
             226125
            ],
            [
             "to",
             205434
            ],
            [
             "a",
             168199
            ]
           ],
           [
            [
             "corallodendron.",
             1
            ],
            [
             "abrus",
             1
            ],
            [
             "precatorius.",
             1
            ],
            [
             "terminalis.",
             1
            ],
            [
             "lxii.",
             1
            ]
           ]
          ]
         },
         "header": {
          "values": [
           "Top-5 Unigrams",
           "Bottom-5 Unigrams"
          ]
         },
         "type": "table",
         "uid": "0b46b894-a4d2-11e8-af31-704d7b35a89d"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"4ec0e3c2-a9a9-49dc-847b-6739f840aeb7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4ec0e3c2-a9a9-49dc-847b-6739f840aeb7\", [{\"cells\": {\"values\": [[[\"the\", 435046], [\"and\", 242206], [\"of\", 226125], [\"to\", 205434], [\"a\", 168199]], [[\"corallodendron.\", 1], [\"abrus\", 1], [\"precatorius.\", 1], [\"terminalis.\", 1], [\"lxii.\", 1]]]}, \"header\": {\"values\": [\"Top-5 Unigrams\", \"Bottom-5 Unigrams\"]}, \"type\": \"table\", \"uid\": \"0b46b895-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"4ec0e3c2-a9a9-49dc-847b-6739f840aeb7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4ec0e3c2-a9a9-49dc-847b-6739f840aeb7\", [{\"cells\": {\"values\": [[[\"the\", 435046], [\"and\", 242206], [\"of\", 226125], [\"to\", 205434], [\"a\", 168199]], [[\"corallodendron.\", 1], [\"abrus\", 1], [\"precatorius.\", 1], [\"terminalis.\", 1], [\"lxii.\", 1]]]}, \"header\": {\"values\": [\"Top-5 Unigrams\", \"Bottom-5 Unigrams\"]}, \"type\": \"table\", \"uid\": \"0b46b895-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import ast\n",
    "from ipy_table import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "from plotly.graph_objs import *\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "regex_str = [\n",
    "            r\"[\\w]+\\-[\\w]+\\-[\\w]+\\-[\\w]+\\-[\\w]+\",          # HYPHEN\n",
    "            r\"\\w+(?:-\\w+)+\",\n",
    "            r\"[\\w]+\\'[\\w]+\",          #APOSTROPHE  \n",
    "            r\"\\b[A-Z][a-zA-Z\\.]*[A-Z]\\b\\.?\",        # Abbreviations\n",
    "            r\"[a-zA-Z]+\",  #normal words\n",
    "            r\"[^\\s\\,\\?\\!\\-\\_\\:\\\"\\'\\;]+\", \n",
    "            r\"[^\\s]\", #anything but space\n",
    "]\n",
    "\n",
    "sentences_str = [\n",
    "        r\"([A-Z][^\\.!?]*[\\.!?])\"\n",
    "]\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "sentences_re = re.compile(r'('+'|'.join(sentences_str)+')',re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def sentence_tokenize(s):\n",
    "    return sentences_re.findall(s)\n",
    "    \n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "arr = [] \n",
    "path = './../Gutenberg/txt/'\n",
    "\n",
    "tokens_i_do_not_want = {\",\",'\"',\" \",\"-\",\"!\",\";\",\"_\",\":\"}\n",
    "spelling_unigrams = {}\n",
    "spelling_bigrams = {}\n",
    "spelling_trigrams = {}\n",
    "unigrams = {}\n",
    "bigrams = {}\n",
    "trigrams = {}\n",
    "kn_unigram_probs = {}\n",
    "kn_bigram_probs = {}\n",
    "lp_bigram_probs = {}\n",
    "count = 0 \n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "    count = count + 1\n",
    "    if count <= 100:\n",
    "        f1 = open(filename, 'r')\n",
    "        varLine = f1.read()\n",
    "        bArray = varLine.replace(\"\\n\",\" \")\n",
    "        bArray = sentence_tokenize(bArray)\n",
    "\n",
    "        for sentences in bArray:\n",
    "            l = preprocess(sentences[0])\n",
    "\n",
    "            for word in l:\n",
    "                word = word.lower()\n",
    "                if word not in tokens_i_do_not_want:\n",
    "                    if word not in unigrams:\n",
    "                        unigrams[word] = 0\n",
    "                    unigrams[word] += 1\n",
    "                    for index,elements in enumerate(word):\n",
    "                        \"\"\" bigram  frequency \"\"\"\n",
    "                        if index < len(word) - 1:\n",
    "                            w1 = word[index].lower()\n",
    "                            w2 = word[index+1].lower()\n",
    "                            spelling_gram = w1 + w2\n",
    "                            if spelling_gram in spelling_bigrams:\n",
    "                                spelling_bigrams[ spelling_gram ] = spelling_bigrams[ spelling_gram ] + 1\n",
    "                            else:\n",
    "                                spelling_bigrams[ spelling_gram ] = 1\n",
    "                           \n",
    "                            if word[index] not in spelling_unigrams:\n",
    "                                spelling_unigrams[word[index]] = 0\n",
    "                            spelling_unigrams[word[index]] += 1\n",
    "                        else:\n",
    "                            if word[index] not in spelling_unigrams:\n",
    "                                spelling_unigrams[word[index]] = 0\n",
    "                            spelling_unigrams[word[index]] += 1\n",
    "                        \n",
    "                        \"\"\" trigram frequency\"\"\"\n",
    "                        \n",
    "                        if index < len(word) - 2:\n",
    "                            w1 = word[index].lower()\n",
    "                            w2 = word[index+1].lower()\n",
    "                            w3 = word[index+2].lower()\n",
    "                            spelling_gram = w1 + w2 + w3\n",
    "                            if spelling_gram in spelling_trigrams:\n",
    "                                spelling_trigrams[ spelling_gram ] = spelling_trigrams[ spelling_gram ] + 1\n",
    "                            else:\n",
    "                                spelling_trigrams[ spelling_gram ] = 1\n",
    "                    \n",
    "                    \n",
    "            for index, word in enumerate(l):\n",
    "                if index < len(l) - 1:\n",
    "                    # we only look at indices up to the\n",
    "                    # next-to-last word, as this is\n",
    "                    # the last one at which a bigram starts\n",
    "                    w1 = l[index].lower()\n",
    "                    w2 = l[index + 1].lower()\n",
    "                    # bigram is a tuple,\n",
    "                    # like a list, but fixed.\n",
    "                    # Tuples can be keys in a dictionary\n",
    "                    if w1 not in tokens_i_do_not_want and w2 not in tokens_i_do_not_want:\n",
    "                        bigram = w1 + \" \" + w2\n",
    "                        if bigram in bigrams:\n",
    "                            bigrams[ bigram ] = bigrams[ bigram ] + 1\n",
    "                        else:\n",
    "                            bigrams[ bigram ] = 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "\n",
    "sorted_tokens = sorted(unigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "trace = go.Table(\n",
    "    header=dict(values=['Top-5 Unigrams', 'Bottom-5 Unigrams']),\n",
    "    cells=dict(values=[sorted_tokens[0:5],\n",
    "                       sorted_tokens[-5:]]))\n",
    "\n",
    "data = [trace] \n",
    "iplot(data, filename = 'basic_table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Plot of Unigrams vs log of their frequency </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr1 = [log(z,10) for m,z in sorted_tokens]\n",
    "boom1 = [m for m,z in sorted_tokens] \n",
    "iplot([{\"x\" : Arr1, \"y\": boom1}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>For the Language model, I've taken a bi-gram LM and applied Knesser Ney smoothing to predict the likelihood of the all the bi-grams</h2>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneser_ney(bigram):\n",
    "    \n",
    "    D = 0.75\n",
    "    u_sum = sum(unigrams.values())\n",
    "    \"\"\"Finding knesser ney probabilities for unigrams\"\"\"\n",
    "    for x in unigrams:\n",
    "        kn_unigram_probs[x] = unigrams[x]/u_sum\n",
    "        \n",
    "    \"\"\"Finding knesser ney probabilities for Bigrams\"\"\"\n",
    "\n",
    "    arr = bigrams.keys()\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    first_word = bigram.split(\" \")[0]\n",
    "    second_word = bigram.split(\" \")[1]\n",
    "    for x in arr:\n",
    "        abc = x.split()\n",
    "        if second_word == abc[1]:\n",
    "            count += 1\n",
    "        if first_word == abc[0]:\n",
    "            count2 += 1\n",
    "            \n",
    "    count1 = unigrams[first_word]\n",
    "    lamda = D*count2/count1\n",
    "\n",
    "\n",
    "    if bigrams[bigram] == 0:\n",
    "        kn_bigram_probs[bigram] = (lamda*count)/len(bigrams)\n",
    "    else: \n",
    "        kn_bigram_probs[bigram] = max((bigrams[bigram] - D),0)/unigrams[first_word]\n",
    "        \n",
    "    return kn_bigram_probs[bigram]  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The possibility of the bigram : 6.825226256677224e-05\n"
     ]
    }
   ],
   "source": [
    "def language_model(bigrams,bigram):\n",
    "# Build your language model\n",
    "# For any language model you build, print a sample of the top 10 and the bottom 10 frequently occuring N-grams.\n",
    "# For any language model you build, display plots of N-Gram vs Freq sorted by frequency (descending) and log-log plots of the same\n",
    "    if bigram in bigrams:\n",
    "        prob = kneser_ney(bigram)\n",
    "    else:\n",
    "        bigrams[bigram] = 0\n",
    "        prob = kneser_ney(bigram)\n",
    "    return prob\n",
    "    \n",
    "print(\"The possibility of the bigram :\",language_model(bigrams,\"who what\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2> Table consisting of the Top-10 and the Bottom-10 frequently occuring bi-grams. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            [
             "of the",
             55142
            ],
            [
             "in the",
             33154
            ],
            [
             "to the",
             24097
            ],
            [
             "and the",
             16837
            ],
            [
             "it was",
             14797
            ],
            [
             "on the",
             14794
            ],
            [
             "to be",
             13514
            ],
            [
             "at the",
             11857
            ],
            [
             "he had",
             11596
            ],
            [
             "he was",
             11536
            ]
           ],
           [
            [
             "} dracaena",
             1
            ],
            [
             "dracaena terminalis.",
             1
            ],
            [
             "} directions",
             1
            ],
            [
             "for preparing",
             1
            ],
            [
             "international exhibition",
             1
            ],
            [
             "of 1862.",
             1
            ],
            [
             "establish and",
             1
            ],
            [
             "one square",
             1
            ],
            [
             "in cacao",
             1
            ],
            [
             "who what",
             0
            ]
           ]
          ]
         },
         "header": {
          "values": [
           "Top-10 Bigrams",
           "Bottom-10 Bigrams"
          ]
         },
         "type": "table",
         "uid": "1b477dc8-a4d2-11e8-af31-704d7b35a89d"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"301d0b90-6202-417a-8ffc-998d85758b5c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"301d0b90-6202-417a-8ffc-998d85758b5c\", [{\"cells\": {\"values\": [[[\"of the\", 55142], [\"in the\", 33154], [\"to the\", 24097], [\"and the\", 16837], [\"it was\", 14797], [\"on the\", 14794], [\"to be\", 13514], [\"at the\", 11857], [\"he had\", 11596], [\"he was\", 11536]], [[\"} dracaena\", 1], [\"dracaena terminalis.\", 1], [\"} directions\", 1], [\"for preparing\", 1], [\"international exhibition\", 1], [\"of 1862.\", 1], [\"establish and\", 1], [\"one square\", 1], [\"in cacao\", 1], [\"who what\", 0]]]}, \"header\": {\"values\": [\"Top-10 Bigrams\", \"Bottom-10 Bigrams\"]}, \"type\": \"table\", \"uid\": \"1b477dc9-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"301d0b90-6202-417a-8ffc-998d85758b5c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"301d0b90-6202-417a-8ffc-998d85758b5c\", [{\"cells\": {\"values\": [[[\"of the\", 55142], [\"in the\", 33154], [\"to the\", 24097], [\"and the\", 16837], [\"it was\", 14797], [\"on the\", 14794], [\"to be\", 13514], [\"at the\", 11857], [\"he had\", 11596], [\"he was\", 11536]], [[\"} dracaena\", 1], [\"dracaena terminalis.\", 1], [\"} directions\", 1], [\"for preparing\", 1], [\"international exhibition\", 1], [\"of 1862.\", 1], [\"establish and\", 1], [\"one square\", 1], [\"in cacao\", 1], [\"who what\", 0]]]}, \"header\": {\"values\": [\"Top-10 Bigrams\", \"Bottom-10 Bigrams\"]}, \"type\": \"table\", \"uid\": \"1b477dc9-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_bigrams = sorted(bigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "trace = go.Table(\n",
    "    header=dict(values=['Top-10 Bigrams', 'Bottom-10 Bigrams']),\n",
    "    cells=dict(values=[sorted_bigrams[0:10],\n",
    "                       sorted_bigrams[-10:]]))\n",
    "\n",
    "data = [trace] \n",
    "iplot(data, filename = 'basic_table')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2> Graphs of the Top-100 frequently occuring bi-grams along with their n-gram vs frequency plot and log-log plot. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "uid": "1ca4a9d4-a4d2-11e8-af31-704d7b35a89d",
         "x": [
          "of the",
          "in the",
          "to the",
          "and the",
          "it was",
          "on the",
          "to be",
          "at the",
          "he had",
          "he was",
          "of a",
          "it is",
          "of his",
          "in a",
          "for the",
          "with a",
          "with the",
          "from the",
          "by the",
          "had been",
          "was a",
          "that he",
          "that the",
          "i am",
          "i have",
          "and i",
          "in his",
          "there was",
          "into the",
          "for a",
          "have been",
          "that i",
          "did not",
          "one of",
          "i was",
          "out of",
          "and he",
          "and a",
          "all the",
          "as he",
          "a little",
          "as the",
          "and then",
          "she was",
          "of her",
          "she had",
          "was the",
          "as a",
          "i had",
          "to his",
          "they were",
          "is a",
          "you are",
          "was not",
          "would be",
          "the same",
          "upon the",
          "to have",
          "but i",
          "could not",
          "and that",
          "to a",
          "if you",
          "but the",
          "would have",
          "' said",
          "there is",
          "he said",
          "the first",
          "the other",
          "is the",
          "to see",
          "you have",
          "which he",
          "a man",
          "to her",
          "that it",
          "when he",
          "and his",
          "will be",
          "of this",
          "that she",
          "i will",
          "to do",
          "his own",
          "he is",
          "such a",
          "to me",
          "he would",
          "as i",
          "do you",
          "said the",
          "i don't",
          "that you",
          "a few",
          "the old",
          "to make",
          "in this",
          "of my",
          "he could"
         ],
         "y": [
          55142,
          33154,
          24097,
          16837,
          14797,
          14794,
          13514,
          11857,
          11596,
          11536,
          11215,
          11098,
          10267,
          10245,
          10081,
          9401,
          9400,
          9359,
          8646,
          8134,
          7913,
          7896,
          7562,
          7382,
          7370,
          6793,
          6603,
          6133,
          5803,
          5681,
          5534,
          5470,
          5406,
          5327,
          5269,
          5214,
          5083,
          5083,
          5073,
          4950,
          4793,
          4785,
          4775,
          4720,
          4438,
          4437,
          4436,
          4416,
          4412,
          4364,
          4360,
          4294,
          4196,
          4173,
          4090,
          4084,
          3956,
          3910,
          3909,
          3845,
          3822,
          3732,
          3716,
          3683,
          3656,
          3630,
          3590,
          3576,
          3549,
          3528,
          3476,
          3465,
          3445,
          3394,
          3302,
          3291,
          3289,
          3282,
          3260,
          3241,
          3204,
          3199,
          3196,
          3190,
          3186,
          3165,
          3155,
          3136,
          3108,
          3101,
          3044,
          3024,
          3015,
          3006,
          2986,
          2985,
          2964,
          2962,
          2952,
          2950
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"74987b50-dcd3-4396-8cff-a025a20e565c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"74987b50-dcd3-4396-8cff-a025a20e565c\", [{\"x\": [\"of the\", \"in the\", \"to the\", \"and the\", \"it was\", \"on the\", \"to be\", \"at the\", \"he had\", \"he was\", \"of a\", \"it is\", \"of his\", \"in a\", \"for the\", \"with a\", \"with the\", \"from the\", \"by the\", \"had been\", \"was a\", \"that he\", \"that the\", \"i am\", \"i have\", \"and i\", \"in his\", \"there was\", \"into the\", \"for a\", \"have been\", \"that i\", \"did not\", \"one of\", \"i was\", \"out of\", \"and he\", \"and a\", \"all the\", \"as he\", \"a little\", \"as the\", \"and then\", \"she was\", \"of her\", \"she had\", \"was the\", \"as a\", \"i had\", \"to his\", \"they were\", \"is a\", \"you are\", \"was not\", \"would be\", \"the same\", \"upon the\", \"to have\", \"but i\", \"could not\", \"and that\", \"to a\", \"if you\", \"but the\", \"would have\", \"' said\", \"there is\", \"he said\", \"the first\", \"the other\", \"is the\", \"to see\", \"you have\", \"which he\", \"a man\", \"to her\", \"that it\", \"when he\", \"and his\", \"will be\", \"of this\", \"that she\", \"i will\", \"to do\", \"his own\", \"he is\", \"such a\", \"to me\", \"he would\", \"as i\", \"do you\", \"said the\", \"i don't\", \"that you\", \"a few\", \"the old\", \"to make\", \"in this\", \"of my\", \"he could\"], \"y\": [55142, 33154, 24097, 16837, 14797, 14794, 13514, 11857, 11596, 11536, 11215, 11098, 10267, 10245, 10081, 9401, 9400, 9359, 8646, 8134, 7913, 7896, 7562, 7382, 7370, 6793, 6603, 6133, 5803, 5681, 5534, 5470, 5406, 5327, 5269, 5214, 5083, 5083, 5073, 4950, 4793, 4785, 4775, 4720, 4438, 4437, 4436, 4416, 4412, 4364, 4360, 4294, 4196, 4173, 4090, 4084, 3956, 3910, 3909, 3845, 3822, 3732, 3716, 3683, 3656, 3630, 3590, 3576, 3549, 3528, 3476, 3465, 3445, 3394, 3302, 3291, 3289, 3282, 3260, 3241, 3204, 3199, 3196, 3190, 3186, 3165, 3155, 3136, 3108, 3101, 3044, 3024, 3015, 3006, 2986, 2985, 2964, 2962, 2952, 2950], \"type\": \"scatter\", \"uid\": \"1ca4a9d5-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"74987b50-dcd3-4396-8cff-a025a20e565c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"74987b50-dcd3-4396-8cff-a025a20e565c\", [{\"x\": [\"of the\", \"in the\", \"to the\", \"and the\", \"it was\", \"on the\", \"to be\", \"at the\", \"he had\", \"he was\", \"of a\", \"it is\", \"of his\", \"in a\", \"for the\", \"with a\", \"with the\", \"from the\", \"by the\", \"had been\", \"was a\", \"that he\", \"that the\", \"i am\", \"i have\", \"and i\", \"in his\", \"there was\", \"into the\", \"for a\", \"have been\", \"that i\", \"did not\", \"one of\", \"i was\", \"out of\", \"and he\", \"and a\", \"all the\", \"as he\", \"a little\", \"as the\", \"and then\", \"she was\", \"of her\", \"she had\", \"was the\", \"as a\", \"i had\", \"to his\", \"they were\", \"is a\", \"you are\", \"was not\", \"would be\", \"the same\", \"upon the\", \"to have\", \"but i\", \"could not\", \"and that\", \"to a\", \"if you\", \"but the\", \"would have\", \"' said\", \"there is\", \"he said\", \"the first\", \"the other\", \"is the\", \"to see\", \"you have\", \"which he\", \"a man\", \"to her\", \"that it\", \"when he\", \"and his\", \"will be\", \"of this\", \"that she\", \"i will\", \"to do\", \"his own\", \"he is\", \"such a\", \"to me\", \"he would\", \"as i\", \"do you\", \"said the\", \"i don't\", \"that you\", \"a few\", \"the old\", \"to make\", \"in this\", \"of my\", \"he could\"], \"y\": [55142, 33154, 24097, 16837, 14797, 14794, 13514, 11857, 11596, 11536, 11215, 11098, 10267, 10245, 10081, 9401, 9400, 9359, 8646, 8134, 7913, 7896, 7562, 7382, 7370, 6793, 6603, 6133, 5803, 5681, 5534, 5470, 5406, 5327, 5269, 5214, 5083, 5083, 5073, 4950, 4793, 4785, 4775, 4720, 4438, 4437, 4436, 4416, 4412, 4364, 4360, 4294, 4196, 4173, 4090, 4084, 3956, 3910, 3909, 3845, 3822, 3732, 3716, 3683, 3656, 3630, 3590, 3576, 3549, 3528, 3476, 3465, 3445, 3394, 3302, 3291, 3289, 3282, 3260, 3241, 3204, 3199, 3196, 3190, 3186, 3165, 3155, 3136, 3108, 3101, 3044, 3024, 3015, 3006, 2986, 2985, 2964, 2962, 2952, 2950], \"type\": \"scatter\", \"uid\": \"1ca4a9d5-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "uid": "1ca4a9d6-a4d2-11e8-af31-704d7b35a89d",
         "x": [
          "of the",
          "in the",
          "to the",
          "and the",
          "it was",
          "on the",
          "to be",
          "at the",
          "he had",
          "he was",
          "of a",
          "it is",
          "of his",
          "in a",
          "for the",
          "with a",
          "with the",
          "from the",
          "by the",
          "had been",
          "was a",
          "that he",
          "that the",
          "i am",
          "i have",
          "and i",
          "in his",
          "there was",
          "into the",
          "for a",
          "have been",
          "that i",
          "did not",
          "one of",
          "i was",
          "out of",
          "and he",
          "and a",
          "all the",
          "as he",
          "a little",
          "as the",
          "and then",
          "she was",
          "of her",
          "she had",
          "was the",
          "as a",
          "i had",
          "to his",
          "they were",
          "is a",
          "you are",
          "was not",
          "would be",
          "the same",
          "upon the",
          "to have",
          "but i",
          "could not",
          "and that",
          "to a",
          "if you",
          "but the",
          "would have",
          "' said",
          "there is",
          "he said",
          "the first",
          "the other",
          "is the",
          "to see",
          "you have",
          "which he",
          "a man",
          "to her",
          "that it",
          "when he",
          "and his",
          "will be",
          "of this",
          "that she",
          "i will",
          "to do",
          "his own",
          "he is",
          "such a",
          "to me",
          "he would",
          "as i",
          "do you",
          "said the",
          "i don't",
          "that you",
          "a few",
          "the old",
          "to make",
          "in this",
          "of my",
          "he could"
         ],
         "y": [
          4.741482513913603,
          4.520535933136838,
          4.3819629776559985,
          4.226264711895693,
          4.170173673806271,
          4.170085614365889,
          4.130783914588956,
          4.073974819866659,
          4.064308206682959,
          4.062055247375354,
          4.049799277918987,
          4.045244720478146,
          4.011443562022075,
          4.010511962737214,
          4.003503614742536,
          3.973174052682972,
          3.973127853599698,
          3.971229447276241,
          3.9368152311976323,
          3.910304168068569,
          3.8983411657275084,
          3.8974071396615804,
          3.8786366730265165,
          3.868174040859638,
          3.867467487859051,
          3.832061614590727,
          3.81974129727301,
          3.7876729646874923,
          3.76365257056453,
          3.754424789277258,
          3.743039154804933,
          3.7379873263334304,
          3.732876041362706,
          3.7264826967848297,
          3.7217281985727877,
          3.71717102683231,
          3.7061201097027037,
          3.7061201097027037,
          3.7052648623174043,
          3.694605198933568,
          3.6806074289917876,
          3.679881942112862,
          3.6789733759197647,
          3.6739419986340875,
          3.647187297895989,
          3.6470894287165545,
          3.646991537477122,
          3.6450290647211423,
          3.6446355037681526,
          3.639884741916304,
          3.6394864892685854,
          3.6328620401002296,
          3.6228354795215196,
          3.6204483847117084,
          3.611723308007342,
          3.611085733414872,
          3.5972562829251413,
          3.5921767573958667,
          3.5920656704322464,
          3.5848963441374497,
          3.5822906827189933,
          3.571941635074462,
          3.5700757053216043,
          3.566201718854913,
          3.5630061870617937,
          3.5599066250361124,
          3.555094448578319,
          3.55339751012388,
          3.5501059993475925,
          3.5475285764597815,
          3.5410797677766284,
          3.5397032389478253,
          3.537189226243644,
          3.530711837981656,
          3.518777068926774,
          3.517327882294373,
          3.517063873482654,
          3.5161385767170743,
          3.513217600067939,
          3.51067903103221,
          3.5056925074122,
          3.505014240084107,
          3.5046067706419537,
          3.503790683057181,
          3.503245771465112,
          3.5003737143533735,
          3.4989993635801526,
          3.496376054012401,
          3.492481010128876,
          3.491501766237326,
          3.4834446480985353,
          3.480581786829169,
          3.47928731647617,
          3.477988976250889,
          3.475089803389006,
          3.4749443354653877,
          3.47187819930729,
          3.4715850541851894,
          3.4701163531510035,
          3.4698220159781625
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"026e1f71-b25d-42fb-9166-757877f9cab5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"026e1f71-b25d-42fb-9166-757877f9cab5\", [{\"x\": [\"of the\", \"in the\", \"to the\", \"and the\", \"it was\", \"on the\", \"to be\", \"at the\", \"he had\", \"he was\", \"of a\", \"it is\", \"of his\", \"in a\", \"for the\", \"with a\", \"with the\", \"from the\", \"by the\", \"had been\", \"was a\", \"that he\", \"that the\", \"i am\", \"i have\", \"and i\", \"in his\", \"there was\", \"into the\", \"for a\", \"have been\", \"that i\", \"did not\", \"one of\", \"i was\", \"out of\", \"and he\", \"and a\", \"all the\", \"as he\", \"a little\", \"as the\", \"and then\", \"she was\", \"of her\", \"she had\", \"was the\", \"as a\", \"i had\", \"to his\", \"they were\", \"is a\", \"you are\", \"was not\", \"would be\", \"the same\", \"upon the\", \"to have\", \"but i\", \"could not\", \"and that\", \"to a\", \"if you\", \"but the\", \"would have\", \"' said\", \"there is\", \"he said\", \"the first\", \"the other\", \"is the\", \"to see\", \"you have\", \"which he\", \"a man\", \"to her\", \"that it\", \"when he\", \"and his\", \"will be\", \"of this\", \"that she\", \"i will\", \"to do\", \"his own\", \"he is\", \"such a\", \"to me\", \"he would\", \"as i\", \"do you\", \"said the\", \"i don't\", \"that you\", \"a few\", \"the old\", \"to make\", \"in this\", \"of my\", \"he could\"], \"y\": [4.741482513913603, 4.520535933136838, 4.3819629776559985, 4.226264711895693, 4.170173673806271, 4.170085614365889, 4.130783914588956, 4.073974819866659, 4.064308206682959, 4.062055247375354, 4.049799277918987, 4.045244720478146, 4.011443562022075, 4.010511962737214, 4.003503614742536, 3.973174052682972, 3.973127853599698, 3.971229447276241, 3.9368152311976323, 3.910304168068569, 3.8983411657275084, 3.8974071396615804, 3.8786366730265165, 3.868174040859638, 3.867467487859051, 3.832061614590727, 3.81974129727301, 3.7876729646874923, 3.76365257056453, 3.754424789277258, 3.743039154804933, 3.7379873263334304, 3.732876041362706, 3.7264826967848297, 3.7217281985727877, 3.71717102683231, 3.7061201097027037, 3.7061201097027037, 3.7052648623174043, 3.694605198933568, 3.6806074289917876, 3.679881942112862, 3.6789733759197647, 3.6739419986340875, 3.647187297895989, 3.6470894287165545, 3.646991537477122, 3.6450290647211423, 3.6446355037681526, 3.639884741916304, 3.6394864892685854, 3.6328620401002296, 3.6228354795215196, 3.6204483847117084, 3.611723308007342, 3.611085733414872, 3.5972562829251413, 3.5921767573958667, 3.5920656704322464, 3.5848963441374497, 3.5822906827189933, 3.571941635074462, 3.5700757053216043, 3.566201718854913, 3.5630061870617937, 3.5599066250361124, 3.555094448578319, 3.55339751012388, 3.5501059993475925, 3.5475285764597815, 3.5410797677766284, 3.5397032389478253, 3.537189226243644, 3.530711837981656, 3.518777068926774, 3.517327882294373, 3.517063873482654, 3.5161385767170743, 3.513217600067939, 3.51067903103221, 3.5056925074122, 3.505014240084107, 3.5046067706419537, 3.503790683057181, 3.503245771465112, 3.5003737143533735, 3.4989993635801526, 3.496376054012401, 3.492481010128876, 3.491501766237326, 3.4834446480985353, 3.480581786829169, 3.47928731647617, 3.477988976250889, 3.475089803389006, 3.4749443354653877, 3.47187819930729, 3.4715850541851894, 3.4701163531510035, 3.4698220159781625], \"type\": \"scatter\", \"uid\": \"1ca4a9d7-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"026e1f71-b25d-42fb-9166-757877f9cab5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"026e1f71-b25d-42fb-9166-757877f9cab5\", [{\"x\": [\"of the\", \"in the\", \"to the\", \"and the\", \"it was\", \"on the\", \"to be\", \"at the\", \"he had\", \"he was\", \"of a\", \"it is\", \"of his\", \"in a\", \"for the\", \"with a\", \"with the\", \"from the\", \"by the\", \"had been\", \"was a\", \"that he\", \"that the\", \"i am\", \"i have\", \"and i\", \"in his\", \"there was\", \"into the\", \"for a\", \"have been\", \"that i\", \"did not\", \"one of\", \"i was\", \"out of\", \"and he\", \"and a\", \"all the\", \"as he\", \"a little\", \"as the\", \"and then\", \"she was\", \"of her\", \"she had\", \"was the\", \"as a\", \"i had\", \"to his\", \"they were\", \"is a\", \"you are\", \"was not\", \"would be\", \"the same\", \"upon the\", \"to have\", \"but i\", \"could not\", \"and that\", \"to a\", \"if you\", \"but the\", \"would have\", \"' said\", \"there is\", \"he said\", \"the first\", \"the other\", \"is the\", \"to see\", \"you have\", \"which he\", \"a man\", \"to her\", \"that it\", \"when he\", \"and his\", \"will be\", \"of this\", \"that she\", \"i will\", \"to do\", \"his own\", \"he is\", \"such a\", \"to me\", \"he would\", \"as i\", \"do you\", \"said the\", \"i don't\", \"that you\", \"a few\", \"the old\", \"to make\", \"in this\", \"of my\", \"he could\"], \"y\": [4.741482513913603, 4.520535933136838, 4.3819629776559985, 4.226264711895693, 4.170173673806271, 4.170085614365889, 4.130783914588956, 4.073974819866659, 4.064308206682959, 4.062055247375354, 4.049799277918987, 4.045244720478146, 4.011443562022075, 4.010511962737214, 4.003503614742536, 3.973174052682972, 3.973127853599698, 3.971229447276241, 3.9368152311976323, 3.910304168068569, 3.8983411657275084, 3.8974071396615804, 3.8786366730265165, 3.868174040859638, 3.867467487859051, 3.832061614590727, 3.81974129727301, 3.7876729646874923, 3.76365257056453, 3.754424789277258, 3.743039154804933, 3.7379873263334304, 3.732876041362706, 3.7264826967848297, 3.7217281985727877, 3.71717102683231, 3.7061201097027037, 3.7061201097027037, 3.7052648623174043, 3.694605198933568, 3.6806074289917876, 3.679881942112862, 3.6789733759197647, 3.6739419986340875, 3.647187297895989, 3.6470894287165545, 3.646991537477122, 3.6450290647211423, 3.6446355037681526, 3.639884741916304, 3.6394864892685854, 3.6328620401002296, 3.6228354795215196, 3.6204483847117084, 3.611723308007342, 3.611085733414872, 3.5972562829251413, 3.5921767573958667, 3.5920656704322464, 3.5848963441374497, 3.5822906827189933, 3.571941635074462, 3.5700757053216043, 3.566201718854913, 3.5630061870617937, 3.5599066250361124, 3.555094448578319, 3.55339751012388, 3.5501059993475925, 3.5475285764597815, 3.5410797677766284, 3.5397032389478253, 3.537189226243644, 3.530711837981656, 3.518777068926774, 3.517327882294373, 3.517063873482654, 3.5161385767170743, 3.513217600067939, 3.51067903103221, 3.5056925074122, 3.505014240084107, 3.5046067706419537, 3.503790683057181, 3.503245771465112, 3.5003737143533735, 3.4989993635801526, 3.496376054012401, 3.492481010128876, 3.491501766237326, 3.4834446480985353, 3.480581786829169, 3.47928731647617, 3.477988976250889, 3.475089803389006, 3.4749443354653877, 3.47187819930729, 3.4715850541851894, 3.4701163531510035, 3.4698220159781625], \"type\": \"scatter\", \"uid\": \"1ca4a9d7-a4d2-11e8-af31-704d7b35a89d\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iplot([{\"x\" : list(zip(*sorted_bigrams[0:100]))[0], \"y\": list(zip(*sorted_bigrams[0:100]))[1]}])\n",
    "Arr = []\n",
    "count = 0\n",
    "Arr = [log(z,10) for m,z in sorted_bigrams[0:100]]\n",
    "boom = [m for m,z in sorted_bigrams[0:100]]     \n",
    "iplot([{\"x\" : boom, \"y\": Arr}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2>To calculate all the bigram probabilities of the bi-gram model I have used Laplacian smoothing with factor 50 or 100 whichever gives me better results</h2><br/>\n",
    "<h3>This creates a list containing the smoothed bi-gram probabilities</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am : 0.06435985736828799\n",
      "who what 6.769825918762089e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def laplace_bigram_probability(bigrams):\n",
    "    \n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        arr = bigram.split(\" \")\n",
    "        bigram = arr[0].lower() + \" \" + arr[1].lower()\n",
    "        n_bigram = 0\n",
    "        \n",
    "        if bigrams[bigram] != 0:\n",
    "            n_bigram = bigrams[bigram]\n",
    "\n",
    "        first_word=arr[0]\n",
    "        n_first_word=0\n",
    "\n",
    "        if first_word in unigrams:\n",
    "            n_first_word = unigrams[first_word]\n",
    "\n",
    "        V=len(unigrams)\n",
    "        \n",
    "        lp_bigram_probs[bigram] =(n_bigram+0.14*1)/float(n_first_word+100) #a simple add 1 smoothing\n",
    "    \n",
    "    return 1\n",
    "\n",
    "bigram_probability(bigrams)\n",
    "print(\"I am :\",lp_bigram_probs[\"i am\"])\n",
    "bigrams[\"who what\"] = 0 #Unseen word in the dictionary\n",
    "bigram_probability(bigrams)\n",
    "print(\"who what\",lp_bigram_probs[\"who what\"]) # Some mass is alloted to this bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h3>Implementing Backoff (Katz 1987) on bigrams. </h3>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Bigram using back-off and Kneser-Ney :  0.048969704403287136\n"
     ]
    }
   ],
   "source": [
    "backoff_prob = {}\n",
    "def backoff(bigram):\n",
    "    alpha1 = 0.75\n",
    "    alpha2 = 0.25\n",
    "    arr = bigram.split()\n",
    "    if bigrams[bigram] > 0:\n",
    "        language_model(bigrams,bigram)\n",
    "        backoff_prob[bigram] = kn_bigram_probs[bigram]\n",
    "    elif bigrams[bigram] == 0 and unigrams[arr[0]] > 0:\n",
    "        bigrams[bigram] = 0\n",
    "        language_model(bigrams,bigram)\n",
    "        backoff_prob[bigram] = alpha1*kn_bigram_probs[bigram]\n",
    "    else:\n",
    "        language_model(bigrams,bigram)\n",
    "        backoff_prob[bigram] = alpha2*kn_unigram_probs[arr[0]]\n",
    "    \n",
    "    return backoff_prob[bigram]\n",
    "\n",
    "\n",
    "print(\"Probability of Bigram using back-off and Kneser-Ney : \",backoff(\"am i\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2><em>Like the language-model I break the word tokens into trigrams,bigrams and unigrams and using a simple add-one smoothing on this model, I find the likelihood of the word</em>.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.10468576450374\n",
      "Correct Spelling\n"
     ]
    }
   ],
   "source": [
    "def spell_prob(word):\n",
    "    \n",
    "    spelling_tokens = []\n",
    "    for index,y in enumerate(word):\n",
    "        spelling_tokens.append(word[index].lower())\n",
    "    \n",
    "    prev_spelling_token = spelling_tokens[0]+spelling_tokens[1]\n",
    "    new_spelling_trigrams = []\n",
    "    word_likelihood=0\n",
    "    \n",
    "    for index in range(2,len(spelling_tokens)):\n",
    "        next_spelling_token=spelling_tokens[index]\n",
    "        spelling_bigram=prev_spelling_token + next_spelling_token\n",
    "        new_spelling_trigrams.append(spelling_bigram)\n",
    "        prev_spelling_token=spelling_tokens[index-1] + next_spelling_token\n",
    "    \n",
    "    \"\"\"\n",
    "    finding trigram likelihood of the word\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    for spelling_trigram in new_spelling_trigrams:\n",
    "        n_trigram = 0\n",
    "        if spelling_trigram in spelling_trigrams:\n",
    "            n_trigram = spelling_trigrams[spelling_trigram]\n",
    "        \n",
    "        first_bigram=spelling_trigram[index] + spelling_trigram[index+1]\n",
    "        \n",
    "        n_first_letter=0\n",
    "        \n",
    "        if first_bigram in spelling_bigrams:\n",
    "            n_first_letter = spelling_bigrams[first_bigram]\n",
    "        \n",
    "        V=len(spelling_bigrams)\n",
    "        \n",
    "        trigram_likelihood=(n_trigram+1)/float(n_first_letter+100) #a simple add 1 smoothing\n",
    "        word_likelihood += -math.log(trigram_likelihood)\n",
    "        \n",
    "    return word_likelihood\n",
    "\n",
    "print(spell_prob(\"beautiful\"))\n",
    "def spell_checker(word,prob):\n",
    "    \n",
    "    if len(word) == 3:\n",
    "        if prob <= 3.3:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "    if len(word) == 4:\n",
    "        if prob <= 7.1:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "    if len(word) == 5:\n",
    "        if prob <= 8.5:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "    if len(word) == 6:\n",
    "        if prob <= 10:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "    if len(word) == 7:\n",
    "        if prob <= 13.7:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "    if len(word) == 8:\n",
    "        if prob <= 16.5:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "    if len(word) == 9:\n",
    "        if prob <= 19.2:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "    if len(word) == 10:\n",
    "        if prob <= 24:\n",
    "            print(\"Correct Spelling\")\n",
    "        else:\n",
    "            print(\"Incorrect Spelling\")\n",
    "\n",
    "spell_checker(\"wonderful\",spell_prob(\"wonderful\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2><em>I find the set of all those bi-grams which has the last word of the sentence as it first word and simultaneously find the probabilities associated with them to predict the next word.</em></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let me know that :  0.10698671321889246\n",
      "let me know what :  0.09437881873727087\n",
      "let me know the :  0.04714770633304238\n",
      "let me know how :  0.046177868295994566\n",
      "let me know it :  0.03405489283289691\n",
      "let me know you :  0.02775094559208612\n",
      "let me know i :  0.020671127921637084\n",
      "let me know not :  0.019798273688294052\n",
      "let me know where :  0.019604306080884492\n",
      "let me know of :  0.01727669479196974\n"
     ]
    }
   ],
   "source": [
    "tokens_i_do_not_want.add(\"?\")\n",
    "def complete_ngram(ngram,n):\n",
    "    words = ngram.split(\" \")\n",
    "    first_word = words[len(words)-1]\n",
    "    \n",
    "    arr = bigrams.keys()\n",
    "    \n",
    "    poss_bigrams = {}\n",
    "    for x in arr:\n",
    "        y = x.split(\" \")\n",
    "        \n",
    "        if first_word == y[0]:\n",
    "            poss_bigrams[x] = lp_bigram_probs[x]\n",
    "    poss_bigrams = sorted(poss_bigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "    for x in poss_bigrams[:10]:\n",
    "        var = x[0].split(\" \")\n",
    "        if var[1] != words[len(words)-2] and var[1] not in tokens_i_do_not_want:\n",
    "            print(ngram+\" \" +var[1] +\" : \", x[1])\n",
    "       \n",
    "\n",
    "complete_ngram(\"let me know\",4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><em>I have used the Knesser-Ney smoothing on bi-gram model to process the likelihood of the sentence.</em></h2>\n",
    "<p><em>The lesser the score for the sentence the higher is the probability of that sentence being valid.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: apple a have red i : 36.188283950778676\n",
      "S2: I have a red apple : 25.39373651910472\n"
     ]
    }
   ],
   "source": [
    "def score_grammaticality(sentence):\n",
    "    words = preprocess(sentence)\n",
    "    bigram_list = {}\n",
    "    for index,word in enumerate(words):\n",
    "        if index < len(words) - 1:\n",
    "            word1 = words[index].lower()\n",
    "            word2 = words[index+1].lower()\n",
    "            bigram = word1 + \" \" + word2\n",
    "            if bigram not in bigrams:\n",
    "                bigrams[bigram] = 0\n",
    "            bigram_list[bigram] = bigrams[bigram]\n",
    "    \n",
    "    prob = 0\n",
    "    for x in bigram_list:\n",
    "        prob += -math.log(language_model(bigrams,x))\n",
    "    return prob\n",
    "print(\"S1: apple a have red i :\",score_grammaticality(\"apple a have red i\")) \n",
    "print(\"S2: I have a red apple :\",score_grammaticality(\"I have a red apple\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Since \"I have a red apple\" has a lower score than \"apple a have red i\" we can conclude that S1 is worse than S2</b></h3>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
